{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Architecture Exploration\n",
    "\n",
    "This notebook explores the mathematical foundations and implementation details of the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation of Attention\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "The core of attention mechanism is:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$, $K$, $V$ are the query, key, and value matrices\n",
    "- $d_k$ is the dimension of keys\n",
    "- The scaling factor $\\frac{1}{\\sqrt{d_k}}$ prevents large values from making softmax too sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, d_k):\n",
    "    '''Implementation of scaled dot-product attention'''\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Example usage\n",
    "batch_size, seq_len, d_k = 2, 4, 8\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V, d_k)\n",
    "print(f\"Input shapes: Q={Q.shape}, K={K.shape}, V={V.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Causal Attention Implementation\n",
    "\n",
    "In GPT, we need causal (masked) attention where each token can only attend to previous tokens:\n",
    "\n",
    "$$\\text{mask}_{i,j} = \\begin{cases}\n",
    "0 & \\text{if } i \\geq j \\\\\n",
    "-\\infty & \\text{if } i < j\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len, device='cpu'):\n",
    "    '''Create causal mask for GPT'''\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.uint8, device=device))\n",
    "    return mask.unsqueeze(0).unsqueeze(1)  # Add batch and head dimensions\n",
    "\n",
    "# Create a causal mask\n",
    "mask = create_causal_mask(4, device='cpu')\n",
    "print(\"Causal mask:\")\n",
    "print(mask.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "Positional encoding is essential for sequence modeling:\n",
    "\n",
    "$$\\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$\\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    '''Generate positional encoding'''\n",
    "    pos = torch.arange(0, position, dtype=torch.float).unsqueeze(1)\n",
    "    i = torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "    \n",
    "    # Calculate angles\n",
    "    angles = pos / torch.pow(10000, 2 * i / d_model)\n",
    "    \n",
    "    # Apply sin and cos\n",
    "    pe = torch.zeros(position, d_model)\n",
    "    pe[:, 0::2] = torch.sin(angles)  # Even indices\n",
    "    pe[:, 1::2] = torch.cos(angles)  # Odd indices\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate positional encoding\n",
    "pe = positional_encoding(10, 8)\n",
    "print(\"Positional encoding shape:\", pe.shape)\n",
    "print(\"First few rows:\")\n",
    "print(pe[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPT Block Architecture\n",
    "\n",
    "Each GPT block consists of:\n",
    "1. Multi-head attention with residual connection and layer norm\n",
    "2. Feed-forward network with residual connection and layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    '''Single GPT block'''\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with causal mask\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the GPT block\n",
    "block = GPTBlock(512, 8, 2048)\n",
    "x = torch.randn(2, 10, 512)  # batch_size=2, seq_len=10, d_model=512\n",
    "mask = create_causal_mask(10)\n",
    "\n",
    "output = block(x, mask)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Dynamics\n",
    "\n",
    "Understanding how GPT learns to generate text through the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate loss calculation\n",
    "def calculate_loss(predictions, targets):\n",
    "    '''Calculate cross-entropy loss'''\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    batch_size, seq_len, vocab_size = predictions.shape\n",
    "    predictions = predictions.view(-1, vocab_size)\n",
    "    targets = targets.view(-1)\n",
    "    return criterion(predictions, targets)\n",
    "\n",
    "# Example\n",
    "predictions = torch.randn(2, 5, 1000)  # batch_size=2, seq_len=5, vocab_size=1000\n",
    "targets = torch.randint(0, 1000, (2, 5))  # batch_size=2, seq_len=5\n",
    "\n",
    "loss = calculate_loss(predictions, targets)\n",
    "print(f\"Sample loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture Visualization\n",
    "\n",
    "Visualizing how information flows through the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization of the architecture\n",
    "print(\"GPT Architecture Summary:\")\n",
    "print(\"1. Input Embeddings\")\n",
    "print(\"   - Token Embedding\")\n",
    "print(\"   - Positional Encoding\")\n",
    "print(\"2. Transformer Blocks (N layers)\")\n",
    "print(\"   - Multi-head Causal Attention\")\n",
    "print(\"   - Feed-forward Network\")\n",
    "print(\"   - Residual Connections\")\n",
    "print(\"   - Layer Normalization\")\n",
    "print(\"3. Output Layer\")\n",
    "print(\"   - Linear Projection\")\n",
    "print(\"   - Softmax\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}